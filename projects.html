<!DOCTYPE html>
<html>

<head>
    <title>Niall McKinnon</title>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro|Roboto|Roboto+Mono">
    <link rel="stylesheet" href="style.css" type="text/css">
</head>

<body>
    <div class="siteHeader">
        <a class="siteTitle" href="index.html">Niall McKinnon</a>    
        <div class="navBar">
            <a href="projects.html">Projects</a>
            <a href="outreach.html">Outreach</a>
            <a href="./pages/cv.html">CV</a>
            <a href="https://linkedin.com/in/niallmckinnon">LinkedIn</a>
            <a href="mailto:nrm3009@uncw.edu">Contact</a>
            <a href="photography.html">Photography</a>
            <a href="">About</a>
        </div>
    </div>
    
    <div class="section">

        <div class="projectHeader">
            <img src="images/NASA_logo.svg" alt="NASA Meatball Insignia"/>
            <span>
                <div class="headerTitle">Autonomous Rover for Lunar Terrain Mapping</div>   
                <div class="headerSubTitle">NASA Marshall Space Flight Center</div>   
            </span>
        </div>
        
        <div class="summary">     
            <b>Objective: </b> 
            Enable a LiDAR-equipped rover to autonomously create maps of analogue lunar terrain, to be used by subsequent vehicles and astronauts.
        </div>
        
        <p>
            The rover is capable of two simultaneous localization and mapping (SLAM) methods. The first is 3D mapping (Figure 1.1), which
            is incredibly valuable for astronaut EVA planning and identifying areas of scientific interest.
            The second is a much simpler occupancy grid (Figure 1.2), which is more useful for vehicle navigation.
            The 2D map is utilized for hazard avoidance and path planning.
        </p>
        
        <div class="imgGroup">
            <figure>
                <video style="height: 200px;" src="images/3dMapping.mp4" type="video/mp4" autoplay loop muted></video>
                <figcaption>Fig 1.1: 3D point cloud map</figcaption>
            </figure>
            <figure>
                <video style="height: 200px;" src="images/LunarField_Costmap.mp4" type="video/mp4" autoplay loop muted></video>
                <figcaption>Fig 1.2: 2D occupancy grid</figcaption>
            </figure>
        </div>
        
        <p>
            3D map generation was already developed for the Kinematic Navigation and Cartography Knapsack (KNaCK),
            of which this project is an offshoot. My work is focused on the navigation itself, with the goal of complete autonomy.
        </p>
        
        <span class="imgGroup">
            <figure>
                <video style="height: 300px;" src="images/pathfinding.webm" type="video/mp4" autoplay loop muted></video>
                <figcaption>Fig 1.3: Simultaneous Localization & Mapping</figcaption>
            </figure>
        </span>
        
        <p>
            This project makes use of Robot Operating System (ROS) and its Navigation 2 (Nav2) package.
            Nav2 already has sophisticated map generation and path planning capabilities, but it was primarily
            designed for structured environments such as warehouses. Thus, a key challenge of 
            this project is adding robustness for an uneven environment. 
        </p>            
    </div>
    
    <div class="section">
        
        <div class="projectHeader">
            <img src="images/Uncwlogo.svg" alt="UNCW Logo"/>
            <span>
                <div class="headerTitle">Computer Vision and Robot Motion Planning</div>   
                <div class="headerSubTitle">University of North Carolina Wilmington</div>   
            </span>
        </div>
        
        <div class=summary>
            <b>Objective: </b>
            Have a robot identify and locate an object in 3D space, then perform
            appropriate path planning to manipulate the object.
        </div>
        
        <p>
            I had the amazing opportunity to take the inaugural offering of UNCW's introductory robotics course.
            The entire course was focused on a continuous project, designed to give an understanding of robotic systems.
            A general outline of the project is shown below. 
        </p>
        
        <div class="imgGroup">
            <figure>
                <img style="width: 700px;" src="images/objectDetection.gif" type="img/gif" autoplay loop muted></img>
                <figcaption>Fig 2.1: Different methods of object detection</figcaption>
            </figure>
        </div>
        
        <p>
            The hardware consisted of a robot manipulator arm and RGB-Depth (RGBD) camera.
            We knew the characteristics of the object (tennis ball) and could tune our detection programs accordingly.
            Figure 2.1 shows two different methods of object detection. The center panel isolates the ball through
            filtering color values, while the rightmost panel uses shape detection from Python's OpenCV library.
        </p>
        
        <p>
            Once we know the ball's location in the frame, we can isolate the appropriate pixels.
            The depth values within this area are fitted to a sphere to create a model of the ball.
        </p>
        
        <div class="inline">
            <figure>
                <img style="width: 400px;" src="images/incorrectBall.jpg" type="img/jpg"></img>
                <figcaption>Fig 2.2: Model from color isolation.</figcaption>
            </figure>
            <p>
                Figure 2.2 shows the point cloud generated from the camera's color and depth data.
                Areas highlighted in white are those isolated from Figure 2.1. In this image, the ball was isolated
                solely using color values. As we can see, the ball's model
                (shown in red) has been distorted by background areas making it through the filter.
                While the robotic gripper used is large enough to tolerate the resulting offset, a more
                accurate solution would be idea.
            </p>
        </div>
        
        <div class="inline">
            <figure>
                <img style="width: 400px;" src="images/ballModel.jpg" type="img/jpg"></img>
                <figcaption>Fig 2.3: Model from shape detection.</figcaption>
            </figure>
            <p>
                Figure 2.3 shows the vastly superior model generated using shape detection.
                This was done by detecting the ball's shape, and then using a slightly smaller area
                to perform the curve fitting. We can see this as certain yellow areas
                are excluded from the filter. This ensures that no background pixels are mistakenly
                counted as part of the ball.
            </p>
        </div>
        
        <div class="inline">
            <figure>
                <video style="width: 400px;" src="images/robot_gazebo.mov" type="video/mp4" autoplay loop muted></video>
                <figcaption>Fig 2.4: Testing motion planning in Gazebo.</figcaption>
            </figure>
            <p>
                The motion planning itself was fairly straightforward, with our program providing
                a series of waypoints that the robot goes to in sequence.
                One such sequence is shown in Figure 2.4, being simulated in a program called Gazebo.
                The sequence is meant to imitate picking up the ball and then placing it in another location.
            </p>
        </div>
        
        <div class="inline">
            <figure>
                <video style="width: 400px;" src="images/robotArm.MOV" type="video/mp4" autoplay loop muted></video>
                <figcaption>Fig 2.5: Final demonstration.</figcaption>
            </figure>
            <p>
                Finally, the entire system was tested using the real-world robot and camera as shown in Figure 2.5.
                The ball could be placed anywhere on the sheet of paper (limited by camera's FOV at such close range),
                and the robot would locate and move the ball.
            </p>
        </div>
        
    </div>
    
</body>

</html>